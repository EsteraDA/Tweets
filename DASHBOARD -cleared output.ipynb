{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2796290",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cc66aae",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('C:\\\\ESTERA\\\\CCT\\\\TWEETS\\\\sentiment.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e23746b",
   "metadata": {},
   "source": [
    "# EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99cb9239",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1545aac0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c353248",
   "metadata": {},
   "outputs": [],
   "source": [
    "# counting the number of tweets per day\n",
    "tweets_per_day = df['Date'].value_counts().sort_index()\n",
    "\n",
    "# creating a histogram for the distribution of tweets per day\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.hist(tweets_per_day, bins=5, alpha=0.5, color='b', edgecolor='black')\n",
    "plt.xlabel('Number of Tweets Per Day')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Distribution of Tweets Per Day')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5caa91d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# converting the 'Date' column to datetime if it's not already\n",
    "df['Date'] = pd.to_datetime(df['Date'])\n",
    "\n",
    "# extracting the date component and store it in a new column 'DateOnly'\n",
    "df['DateOnly'] = df['Date'].dt.date\n",
    "\n",
    "# finding the oldest date\n",
    "oldest_date = df['DateOnly'].min()\n",
    "\n",
    "# finding the newest date\n",
    "newest_date = df['DateOnly'].max()\n",
    "\n",
    "print(\"Oldest Date:\", oldest_date)\n",
    "print(\"Newest Date:\", newest_date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fad54fdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculating the number of days between the oldest and newest dates\n",
    "days_between = (newest_date - oldest_date).days\n",
    "\n",
    "print(\"Number of days between the oldest and newest date:\", days_between)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fbdf4bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# finding the highest sentiment score\n",
    "highest_sentiment = df['SentimentScore'].max()\n",
    "\n",
    "# finding the lowest sentiment score\n",
    "lowest_sentiment = df['SentimentScore'].min()\n",
    "\n",
    "print(\"Highest Sentiment Score:\", highest_sentiment)\n",
    "print(\"Lowest Sentiment Score:\", lowest_sentiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "127c1b78",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install plotly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71661e26",
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "\n",
    "# creating an interactive box plot\n",
    "fig = px.box(df, x=df['Date'].dt.date, y='SentimentScore', labels={'x': 'Date', 'y': 'Sentiment Score'}, title='Distribution of Sentiment Scores for Each Day')\n",
    "fig.update_xaxes(tickangle=90)\n",
    "\n",
    "# showing the interactive plot\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41136272",
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting the figure size for the plot\n",
    "plt.figure(figsize=(15, 8))\n",
    "\n",
    "# creating the grouped box plot\n",
    "sns.set(style=\"whitegrid\")\n",
    "sns.boxplot(data=df, x=df['Date'].dt.date, y='SentimentScore')\n",
    "\n",
    "# rotating x-axis labels for better readability\n",
    "plt.xticks(rotation=90)\n",
    "\n",
    "# setting labels and title\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Sentiment Score')\n",
    "plt.title('Distribution of Sentiment Scores for Each Day')\n",
    "\n",
    "# showing the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43a756b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# counting the number of rows with sentiment scores lower than -0.7\n",
    "count = (df['SentimentScore'] < -0.7).sum()\n",
    "\n",
    "# calculating the total number of rows in the DataFrame\n",
    "total_rows = len(df)\n",
    "\n",
    "# calculating the percentage\n",
    "percentage = (count / total_rows) * 100\n",
    "\n",
    "print(\"Number of rows with sentiment scores lower than -0.7:\", count, \"which is: {:.2f}%\".format(percentage), 'of entire data.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25505d5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting the date for comparison\n",
    "comparison_date = pd.to_datetime('2009-06-16')\n",
    "\n",
    "# calculating the average sentiment before and after the comparison date\n",
    "before_average = df[df['Date'] < comparison_date]['SentimentScore'].mean()\n",
    "after_average = df[df['Date'] >= comparison_date]['SentimentScore'].mean()\n",
    "\n",
    "# rounding the averages to two decimal places\n",
    "before_average_rounded = round(before_average, 2)\n",
    "after_average_rounded = round(after_average, 2)\n",
    "\n",
    "# printing the results\n",
    "print(f\"The average sentiment before {comparison_date} is: {before_average_rounded}\")\n",
    "print(f\"The average sentiment after {comparison_date} is: {after_average_rounded}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d139918",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Date'] = pd.to_datetime(df['Date'])  \n",
    "df['DayOfWeek'] = df['Date'].dt.strftime('%A') \n",
    "\n",
    "average_sentiment_per_day = df.groupby('DayOfWeek')['SentimentScore'].mean()\n",
    "average_sentiment_per_day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a95686ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculating the average sentiment per day\n",
    "sentiment_per_day = df.groupby('DayOfWeek')['SentimentScore'].mean()\n",
    "\n",
    "# creating a new DataFrame 'df4' with 'DayOfWeek' as index and 'SentimentScore' as the column\n",
    "df4 = sentiment_per_day.reset_index(name='SentimentScore')\n",
    "df4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f11724e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sortting the DataFrame by 'SentimentScore' in descending order\n",
    "df4 = df4.sort_values(by='SentimentScore', ascending=True)\n",
    "\n",
    "# creating an interactive horizontal bar chart\n",
    "fig = px.bar(df4, x='SentimentScore', y='DayOfWeek', orientation='h', title='Average Sentiment Score per Day of the Week')\n",
    "fig.update_layout(xaxis_title='Average Sentiment Score', yaxis_title='Day of the Week', showlegend=False)\n",
    "\n",
    "# showing the interactive chart\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d821cce",
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9f5b0ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import plotly.express as px\n",
    "#import plotly.graph_objects as go\n",
    "\n",
    "# Extract hour and day of the week\n",
    "df['Hour'] = pd.to_datetime(df['Time']).dt.hour\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3ce9e43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# groupping by 'DayOfWeek' and 'Hour', then calculate the average sentiment\n",
    "average_sentiment_per_hour = df.groupby(['DayOfWeek', 'Hour'])['SentimentScore'].mean().reset_index()\n",
    "\n",
    "print(average_sentiment_per_hour)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d602670e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# groupping data by DayOfWeek and Hour, and calculating the average SentimentScore\n",
    "grouped_data = df.groupby(['DayOfWeek', 'Hour'])['SentimentScore'].mean().unstack()\n",
    "\n",
    "# creating the line chart\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "for day, data in grouped_data.iterrows():\n",
    "    ax.plot(data.index, data.values, label=day)\n",
    "\n",
    "ax.set_xlabel('Hour')\n",
    "ax.set_ylabel('Sentiment Score')\n",
    "ax.set_title('Average Sentiment Scores by Hour for Each Day of the Week')\n",
    "ax.legend(loc='upper right')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0318191a",
   "metadata": {},
   "source": [
    "### Checking how many tweets per day there was before sentiment dropped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50feb585",
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining the split date\n",
    "split_date = pd.to_datetime('2009-06-17')\n",
    "\n",
    "# counting the number of tweets for each day before and after the split date\n",
    "tweets_before = len(df[(df['Date'] < split_date)])\n",
    "tweets_after = len(df[(df['Date'] >= split_date)])\n",
    "\n",
    "# calculating the number of days before and after the split date\n",
    "days_before = (split_date - df['Date'].min()).days\n",
    "days_after = (df['Date'].max() - split_date).days\n",
    "\n",
    "# calculating the average number of tweets per day before and after the split date\n",
    "average_tweets_before = tweets_before / days_before\n",
    "average_tweets_after = tweets_after / days_after\n",
    "\n",
    "# printing the results\n",
    "print(\"Average Number of Tweets Per Day Before\", split_date, \":\", average_tweets_before)\n",
    "print(\"Average Number of Tweets Per Day After\", split_date, \":\", average_tweets_after)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d51f3d11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# counting the number of tweets per day and sort by date\n",
    "tweets_per_day = df['Date'].dt.date.value_counts().sort_index().reset_index()\n",
    "tweets_per_day.columns = ['Date', 'Count']\n",
    "\n",
    "# adding a color column based on the date\n",
    "tweets_per_day['Color'] = ['Before 2009-06-17' if date < pd.to_datetime('2009-06-17') else 'After 200-06-17' for date in tweets_per_day['Date']]\n",
    "\n",
    "# creating an interactive bar chart\n",
    "fig = px.bar(tweets_per_day, x='Date', y='Count', color='Color',\n",
    "             title='Number of Tweets Per Day',\n",
    "             labels={'Count': 'Number of Tweets'},\n",
    "             category_orders={'Color': ['skyblue', 'After 2009-06-17']})\n",
    "\n",
    "fig.update_xaxes(title='Date', categoryorder='total ascending')\n",
    "fig.update_yaxes(title='Number of Tweets')\n",
    "\n",
    "# showing the interactive chart\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8629df6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# couting the number of tweets per day\n",
    "#tweets_per_day = df['Date'].dt.date.value_counts().sort_index()\n",
    "\n",
    "# plotting the number of tweets per day\n",
    "#plt.figure(figsize=(10, 6))\n",
    "\n",
    "# defining a color map\n",
    "#colors = ['skyblue' if date < pd.to_datetime('2009-06-17') else 'lightcoral' for date in tweets_per_day.index]\n",
    "\n",
    "# plotting the bars with different colors for dates before and after '2009-06-17'\n",
    "#plt.bar(tweets_per_day.index, tweets_per_day, color=colors)\n",
    "\n",
    "#plt.xlabel('Date')\n",
    "#plt.ylabel('Number of Tweets')\n",
    "#plt.title('Number of Tweets Per Day')\n",
    "##plt.xticks(rotation=90)\n",
    "#plt.tight_layout()\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5387adae",
   "metadata": {},
   "source": [
    "### Average Sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df9ff5b9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# groupping the data by date and calculate the average sentiment score for each day\n",
    "daily_average_sentiment = df.groupby(df['Date'].dt.date)['SentimentScore'].mean()\n",
    "\n",
    "# resetting the index to make the result a DataFrame\n",
    "daily_average_sentiment = daily_average_sentiment.reset_index()\n",
    "\n",
    "# renaming the columns\n",
    "daily_average_sentiment.columns = ['Date', 'AverageSentimentScore']\n",
    "\n",
    "# printing the result\n",
    "print(daily_average_sentiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06512b4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "import dash\n",
    "from dash import dcc, html\n",
    "from dash.dependencies import Input, Output\n",
    "import datetime\n",
    "\n",
    "# Create a Dash app\n",
    "dynamic_chart_app  = dash.Dash(__name__)\n",
    "\n",
    "\n",
    "# Convert the Date column to a datetime object\n",
    "daily_average_sentiment = pd.DataFrame(daily_average_sentiment)\n",
    "daily_average_sentiment['Date'] = pd.to_datetime(daily_average_sentiment['Date'])\n",
    "\n",
    "dynamic_chart_app.layout = html.Div([\n",
    "    html.H1('Average Sentiment Score Over Time'),\n",
    "    dcc.Graph(id='line-chart'),\n",
    "])\n",
    "\n",
    "@app.callback(\n",
    "    Output('line-chart', 'figure'),\n",
    "    Input('line-chart', 'relayoutData')\n",
    ")\n",
    "def update_line_chart(relayoutData):\n",
    "    # Filter the data based on zoom level\n",
    "    if relayoutData and 'xaxis.range' in relayoutData:\n",
    "        start_date = datetime.datetime.fromisoformat(relayoutData['xaxis.range'][0])\n",
    "        end_date = datetime.datetime.fromisoformat(relayoutData['xaxis.range'][1])\n",
    "        filtered_df = daily_average_sentiment[(daily_average_sentiment['Date'] >= start_date) & (daily_average_sentiment['Date'] <= end_date)]\n",
    "    else:\n",
    "        filtered_df = daily_average_sentiment\n",
    "\n",
    "    fig = px.line(filtered_df, x='Date', y='AverageSentimentScore', title='Average Sentiment Score Over Time')\n",
    "    return fig\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    dynamic_chart_app.run_server(debug=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d52c7bcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# groupping the data by date and calculating the average sentiment score for each day\n",
    "daily_average_sentiment = df.groupby(df['Date'].dt.date)['SentimentScore'].mean()\n",
    "\n",
    "# resetting the index to make the result a DataFrame\n",
    "daily_average_sentiment = daily_average_sentiment.reset_index()\n",
    "\n",
    "# renaming the columns\n",
    "daily_average_sentiment.columns = ['Date', 'AverageSentimentScore']\n",
    "\n",
    "# creating an interactive line chart with a slider\n",
    "fig = px.line(daily_average_sentiment, x='Date', y='AverageSentimentScore',\n",
    "              title='Daily Average Sentiment Score',\n",
    "              labels={'AverageSentimentScore': 'Average Sentiment Score'},\n",
    "              range_x=['start_date', 'end_date']) \n",
    "\n",
    "fig.update_xaxes(title='Date')\n",
    "fig.update_yaxes(title='Average Sentiment Score')\n",
    "\n",
    "# showing the interactive chart\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03825b98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating a time series plot to visualize the daily sentiment trends\n",
    "#plt.figure(figsize=(12, 6))\n",
    "#plt.plot(daily_average_sentiment['Date'], daily_average_sentiment['AverageSentimentScore'], label='Average Sentiment Score', color='b')\n",
    "#plt.xlabel('Date')\n",
    "#plt.ylabel('Average Sentiment Score')\n",
    "#plt.title('Daily Average Sentiment Score Over Time')\n",
    "#plt.grid(True)\n",
    "#plt.legend()\n",
    "#plt.xticks(rotation=45)\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8cd2b37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculating the average sentiment\n",
    "average_sentiment = daily_average_sentiment['AverageSentimentScore'].mean()\n",
    "\n",
    "# rounding it to two decimal places\n",
    "average_sentiment_rounded = round(average_sentiment, 2)\n",
    "\n",
    "# printing the result\n",
    "print(f\"The average sentiment is: {average_sentiment_rounded}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ccd774c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# converting 'Date' to datetime\n",
    "daily_average_sentiment['Date'] = pd.to_datetime(daily_average_sentiment['Date'])\n",
    "\n",
    "# filtering data before and after 2009-06-17\n",
    "before_date = pd.to_datetime('2009-06-17')\n",
    "before_average_sentiment = daily_average_sentiment[daily_average_sentiment['Date'] < before_date]['AverageSentimentScore'].mean()\n",
    "after_average_sentiment = daily_average_sentiment[daily_average_sentiment['Date'] >= before_date]['AverageSentimentScore'].mean()\n",
    "\n",
    "# rounding the average sentiment scores\n",
    "before_average_sentiment = round(before_average_sentiment, 2)\n",
    "after_average_sentiment = round(after_average_sentiment, 2)\n",
    "\n",
    "# printing the results\n",
    "print(f\"Average sentiment score before {before_date}: {before_average_sentiment}\")\n",
    "print(f\"Average sentiment score after {before_date}: {after_average_sentiment}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d513be5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "# Define the data for the pie chart (labels, values, and colors)\n",
    "labels = ['', 'Negative', '', '', '', '', '', '', '', '', '', '', '', '', '', 'Positive', '']\n",
    "values = [0.5, 0.03125, 0.03125, 0.03125, 0.03125, 0.03125, 0.03125, 0.03125, 0.03125, 0.03125,\n",
    "          0.03125, 0.03125, 0.03125, 0.03125, 0.03125, 0.03125, 0.03125]\n",
    "colors = ['none', '#FF0000', '#FF2200', '#FF4400', '#FF6600', '#FF8800', '#FFAA00', '#FFCC00', '#FFFF33',\n",
    "          '#FFFF66', '#FFFF99', '#FFFFCC', '#CCFFCC', '#99FF99', '#66FF66', '#33FF33', '#00FF00']\n",
    "\n",
    "# Create the pie chart\n",
    "fig, ax = plt.subplots()\n",
    "ax.pie(values, labels=labels, colors=colors, startangle=0, counterclock=False, radius=1.5, wedgeprops={'width': 0.4})\n",
    "ax.axis('equal')\n",
    "ax.set_title('Tweets average Sentiment')\n",
    "\n",
    "# Add lines to the chart\n",
    "x1, y1 = 1, 0.01\n",
    "angle_rad = np.radians(86)\n",
    "x = [0, 0.8 * np.cos(angle_rad)]\n",
    "y = [0, 0.8 * np.sin(angle_rad)]\n",
    "ax.plot(x, y, color='black', linestyle='--', linewidth=1)\n",
    "\n",
    "# Add the additional points with custom coordinates\n",
    "offset = 0.05\n",
    "ax.annotate('0.14', xy=(x[1], y[1] + offset), fontsize=12, color='blue', ha='center', va='center')\n",
    "ax.annotate('1', xy=(x1, y1), fontsize=12, color='black')\n",
    "ax.annotate('-1', xy=(-1, 0.01), fontsize=12, color='black')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcef0cc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tkinter as tk\n",
    "from matplotlib.backends.backend_tkagg import FigureCanvasTkAgg\n",
    "from matplotlib.figure import Figure\n",
    "\n",
    "\n",
    "# creating a function to generate the pie chart with lines and additional points\n",
    "def create_pie_chart(title, angle, x1, y1, value_1, value_minus_1):\n",
    "    fig = Figure(figsize=(5, 5))\n",
    "    ax = fig.add_subplot(111)\n",
    "    \n",
    "    # defining the labels and values for the pie chart\n",
    "    labels = ['', 'Negative', '', '', '', '', '', '', '', '', '', '', '', '', '', 'Positive', '']\n",
    "    values = [0.5, 0.03125, 0.03125, 0.03125, 0.03125, 0.03125, 0.03125, 0.03125, 0.03125, 0.03125,\n",
    "              0.03125, 0.03125, 0.03125, 0.03125, 0.03125, 0.03125, 0.03125]\n",
    "    \n",
    "    # defining the colors for each segment\n",
    "    colors = ['none', '#FF0000', '#FF2200', '#FF4400', '#FF6600', '#FF8800', '#FFAA00', '#FFCC00', '#FFFF33',\n",
    "              '#FFFF66', '#FFFF99', '#FFFFCC', '#CCFFCC', '#99FF99', '#66FF66', '#33FF33', '#00FF00']\n",
    "    \n",
    "    # creating a pie chart\n",
    "    ax.pie(values, labels=labels, colors=colors, startangle=0, counterclock=False, radius=1.5, wedgeprops={'width': 0.4})\n",
    "    ax.axis('equal')\n",
    "    ax.set_title(title)\n",
    "    \n",
    "    # adding lines to the chart\n",
    "    ax.annotate('1', xy=(x1, y1), fontsize=12, color='black')\n",
    "    ax.annotate('-1', xy=(-1, 0.01), fontsize=12, color='black')\n",
    "    \n",
    "    angle_rad = np.radians(angle)\n",
    "    x = [0, 0.8 * np.cos(angle_rad)]\n",
    "    y = [0, 0.8 * np.sin(angle_rad)]\n",
    "    ax.plot(x, y, color='black', linestyle='--', linewidth=1)\n",
    "    \n",
    "    # adding the additional points with custom coordinates\n",
    "    x2 = x[1]  \n",
    "    y2 = y[1] \n",
    "    offset = 0.05  # Adjust the offset to raise the blue values\n",
    "    ax.annotate(value_1, xy=(x2, y2 + offset), fontsize=12, color='blue', ha='center', va='center')\n",
    "    \n",
    "    x3 = x[1]  \n",
    "    y3 = y[1]  \n",
    "    offset = -0.05\n",
    "  #  ax.annotate(value_minus_1, xy=(x3, y3 + offset), fontsize=12, color='red', ha='center', va='center')\n",
    "    \n",
    "    return fig\n",
    "\n",
    "# creating the main application window\n",
    "root = tk.Tk()\n",
    "root.title('Interactive Pie Charts')\n",
    "\n",
    "# creating three separate pie charts with different titles, angles, and line coordinates\n",
    "chart1 = create_pie_chart('Total Average', 90, 1, 0.01, '0.14', '-1')\n",
    "chart2 = create_pie_chart('Before 2009-06-17', 85, 1, 0.01, '0.17', '-1')\n",
    "chart3 = create_pie_chart('After 2009-06-17', 95, 1, 0.01, '-0.05', '-1')\n",
    "\n",
    "# creating canvas widgets for the charts\n",
    "canvas1 = FigureCanvasTkAgg(chart1, master=root)\n",
    "canvas2 = FigureCanvasTkAgg(chart2, master=root)\n",
    "canvas3 = FigureCanvasTkAgg(chart3, master=root)\n",
    "\n",
    "# packing the canvas widgets to the window\n",
    "canvas1.get_tk_widget().pack(side=tk.LEFT, fill=tk.BOTH, expand=True)\n",
    "canvas2.get_tk_widget().pack(side=tk.LEFT, fill=tk.BOTH, expand=True)\n",
    "canvas3.get_tk_widget().pack(side=tk.LEFT, fill=tk.BOTH, expand=True)\n",
    "\n",
    "# startig the Tkinter\n",
    "root.mainloop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7801d676",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# function to create a pie chart\n",
    "def create_pie_chart(ax, title, angle, value_1, value_minus_1):\n",
    "    \n",
    "    # defining the data for the pie chart (labels, values, and colors)\n",
    "    labels = ['', 'Negative', '', '', '', '', '', '', '', '', '', '', '', '', '', 'Positive', '']\n",
    "    values = [0.5, 0.03125, 0.03125, 0.03125, 0.03125, 0.03125, 0.03125, 0.03125, 0.03125, 0.03125,\n",
    "              0.03125, 0.03125, 0.03125, 0.03125, 0.03125, 0.03125, 0.03125]\n",
    "    colors = ['none', '#FF0000', '#FF2200', '#FF4400', '#FF6600', '#FF8800', '#FFAA00', '#FFCC00', '#FFFF33',\n",
    "              '#FFFF66', '#FFFF99', '#FFFFCC', '#CCFFCC', '#99FF99', '#66FF66', '#33FF33', '#00FF00']\n",
    "\n",
    "    # C+creating the pie chart\n",
    "    ax.pie(values, labels=labels, colors=colors, startangle=0, counterclock=False, radius=1.5, wedgeprops={'width': 0.4})\n",
    "    ax.axis('equal')\n",
    "    ax.set_title(title)\n",
    "\n",
    "    # adding lines to the chart\n",
    "    x1, y1 = 1, 0.01\n",
    "    angle_rad = np.radians(angle)\n",
    "    x = [0, 0.8 * np.cos(angle_rad)]\n",
    "    y = [0, 0.8 * np.sin(angle_rad)]\n",
    "    ax.plot(x, y, color='black', linestyle='--', linewidth=1)\n",
    "\n",
    "    # adding the additional points with custom coordinates\n",
    "    offset = 0.05 \n",
    "    ax.annotate(value_1, xy=(x[1], y[1] + offset), fontsize=12, color='blue', ha='center', va='center')\n",
    "    ax.annotate('1', xy=(x1, y1), fontsize=12, color='black')\n",
    "    ax.annotate('-1', xy=(-1, 0.01), fontsize=12, color='black')\n",
    "\n",
    "# creating a figure with subplots for the pie charts\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "\n",
    "# creating pie charts and add them to the subplots\n",
    "create_pie_chart(axes[0], 'Total Average', 90, '0.14', '-1')\n",
    "create_pie_chart(axes[1], 'Before 2009-06-17', 85, '0.17', '-1')\n",
    "create_pie_chart(axes[2], 'After 2009-06-17', 95, '-0.05', '-1')\n",
    "\n",
    "\n",
    "fig.savefig(r'C:\\\\ESTERA\\\\CCT\\\\TWEETS\\\\pie_charts.png', bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d73e8a1",
   "metadata": {},
   "source": [
    "# Checking tweets content for context of the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94975169",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "csv_file_path = \"C:\\\\ESTERA\\\\CCT\\\\TWEETS\\\\ProjectTweets.csv\"\n",
    "column_names =['Id','Tweet_id','Date','Flag','User','Tweet']\n",
    "\n",
    "data_2 = pd.read_csv(csv_file_path, names=column_names)\n",
    "data_2.columns = column_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1005d9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd112b47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking for duplicates\n",
    "duplicates = data_2[data_2.duplicated()]\n",
    "\n",
    "if duplicates.empty:\n",
    "    print(\"No duplicates found in the DataFrame.\")\n",
    "else:\n",
    "    print(\"Duplicates found in the DataFrame:\")\n",
    "    print(duplicates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5b48a8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_2.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f67d03cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# removing the 'PDT' part from the 'Date' column\n",
    "data_2['Date'] = data_2['Date'].str.replace(' PDT', '')\n",
    "\n",
    "# converting the 'Date' column to a datetime format\n",
    "data_2['Date'] = pd.to_datetime(data_2['Date'], format='%a %b %d %H:%M:%S %Y')\n",
    "\n",
    "# extracting the 'Time' component while preserving the original date format\n",
    "data_2['Time'] = data_2['Date'].dt.strftime('%H:%M:%S')\n",
    "\n",
    "# extracting the 'Date' component in the 'YYYY-MM-DD' format\n",
    "data_2['Date'] = data_2['Date'].dt.strftime('%Y-%m-%d')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cc6673b",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_2.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "868c08b7",
   "metadata": {},
   "source": [
    "### preprocessing data before pulling context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1be198d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "nltk.download('stopwords')\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "def preprocess_text(text):\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    tokens = [word.lower() for word in tokens if word.isalpha()]\n",
    "    tokens = [word for word in tokens if word not in stop_words]\n",
    "    return \" \".join(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2aaf1136",
   "metadata": {},
   "outputs": [],
   "source": [
    "# appling the preprocessing function to the 'Tweet' column\n",
    "data_2['Tweet'] = data_2['Tweet'].apply(preprocess_text)\n",
    "data_2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "902dce1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "\n",
    "# creating a Document-Term Matrix (DTM)\n",
    "vectorizer = CountVectorizer(max_features=1000, stop_words='english')\n",
    "dtm = vectorizer.fit_transform(data_2['Tweet'])\n",
    "\n",
    "# appling LDA\n",
    "num_topics = 5 \n",
    "lda = LatentDirichletAllocation(n_components=num_topics, random_state=42)\n",
    "lda.fit(dtm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6feef5e",
   "metadata": {},
   "source": [
    "#### Inspecting topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e19ff06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# getting the top 10 words for each topic\n",
    "num_top_words = 10  \n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "for topic_idx, topic in enumerate(lda.components_):\n",
    "    top_words_idx = topic.argsort()[:-num_top_words - 1:-1]\n",
    "    top_words = [feature_names[i] for i in top_words_idx]\n",
    "    print(f\"Topic {topic_idx + 1}: {', '.join(top_words)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d6ef563",
   "metadata": {},
   "source": [
    "###\n",
    "Topic 1: Positive Emotions <br>\n",
    "Topic 2: Daily Life and Activities<br>\n",
    "Topic 3: Online Interaction and Communication <br>\n",
    "Topic 4: Daily Routine and Well-being <br>\n",
    "Topic 5: Expressing Thoughts and Opinions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e3bdffc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# assigning topics to documents\n",
    "topic_distribution = lda.transform(dtm)\n",
    "\n",
    "# adding 1 to topic labels to make them start from 1\n",
    "data_2['Topic'] = (topic_distribution.argmax(axis=1) + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7e29b13",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_2.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6935d3e9",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# groupping by 'Date' and 'Topic', and count the number of occurrences\n",
    "topic_counts = data_2.groupby(['Date', 'Topic']).size().unstack(fill_value=0)\n",
    "\n",
    "# creating a stacked area plot\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.set_style(\"whitegrid\")\n",
    "topic_counts.plot(kind='area', stacked=True, colormap='viridis')\n",
    "plt.title(\"Topic Distribution Over Time\")\n",
    "plt.xlabel(\"Date\")\n",
    "plt.ylabel(\"Number of Tweets\")\n",
    "plt.legend(title='Topic', loc='upper right', labels=['', 'Topic 1', '', 'Topic 2', '', 'Topic 3','','Topic 4','','Topic 5'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04c860dc",
   "metadata": {},
   "source": [
    "# Sentiment Forecasting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "493d5167",
   "metadata": {},
   "source": [
    "### 1. Analizing missing data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8eede8b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(daily_average_sentiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5d920d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# converting the 'Date' column to datetime\n",
    "daily_average_sentiment['Date'] = pd.to_datetime(daily_average_sentiment['Date'])\n",
    "\n",
    "# finding the minimum and maximum dates in DataFrame\n",
    "min_date = daily_average_sentiment['Date'].min()\n",
    "max_date = daily_average_sentiment['Date'].max()\n",
    "\n",
    "# creating a date range covering the entire range\n",
    "date_range = pd.date_range(start=min_date, end=max_date)\n",
    "\n",
    "# creating a new DataFrame with the date range\n",
    "date_range_df = pd.DataFrame({'Date': date_range})\n",
    "\n",
    "# merging the date range DataFrame with 'daily_average_sentiment' DataFrame, filling missing dates\n",
    "merged_data = date_range_df.merge(daily_average_sentiment, on='Date', how='left')\n",
    "\n",
    "# setting 'AverageSentimentScore' to NaN for missing dates\n",
    "merged_data['AverageSentimentScore'].fillna(float('nan'), inplace=True)\n",
    "\n",
    "# sortting the merged DataFrame by date\n",
    "merged_data.sort_values('Date', inplace=True)\n",
    "\n",
    "# reseting the index\n",
    "merged_data.reset_index(drop=True, inplace=True)\n",
    "merged_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0973561f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "merged_data.head(40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "147edbfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_data = pd.DataFrame(merged_data)\n",
    "\n",
    "# setting 'Date' as the index\n",
    "missing_data.set_index('Date', inplace=True)\n",
    "\n",
    "# date to check before and after\n",
    "date_to_check = '2009-05-09'\n",
    "\n",
    "# counting NaN values before and after the date\n",
    "nan_before = missing_data.loc[:date_to_check, 'AverageSentimentScore'].isna().sum()\n",
    "nan_after = missing_data.loc[date_to_check:, 'AverageSentimentScore'].isna().sum()\n",
    "\n",
    "print(\"Number of NaN values before\", date_to_check, \":\", nan_before)\n",
    "print(\"Number of NaN values after\", date_to_check, \":\", nan_after)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f5d3455",
   "metadata": {},
   "source": [
    "### 2. Handling missing data\n",
    "Data will be temporary tuncated as majority of missing data points are located at very begining of data set."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "318eaa29",
   "metadata": {},
   "source": [
    "#### a) filling missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a75c1b2e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# selecting only rows after '2009-05-09'\n",
    "rows = missing_data.loc['2009-05-09':]\n",
    "print(rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db5dfcf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# filling missing values using linear interpolation\n",
    "rows['AverageSentimentScore'].interpolate(method='linear', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "189b315f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(rows)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fa1c545",
   "metadata": {},
   "source": [
    "#### b) checking trend, seasonality and randomness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25d2701e",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "from dateutil.parser import parse\n",
    "%matplotlib inline\n",
    "\n",
    "# Additive Decomposition\n",
    "result_add = seasonal_decompose(rows['AverageSentimentScore'], model='additive', extrapolate_trend='freq')\n",
    "\n",
    "# Plot\n",
    "plt.rcParams.update({'figure.figsize': (10,10)})\n",
    "result_add.plot().suptitle('Additive Decompose', fontsize=22)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7575bf6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the Components \n",
    "df_reconstructed = pd.concat([result_add.seasonal, result_add.trend, result_add.resid, result_add.observed], axis=1)\n",
    "df_reconstructed.columns = ['seas', 'trend', 'resid', 'actual_values']\n",
    "df_reconstructed.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1e6e802",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(df_reconstructed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d946dfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating a figure and plot\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(df_reconstructed.index, df_reconstructed['seas'], label='Seasonal', marker='o')\n",
    "plt.plot(df_reconstructed.index, df_reconstructed['trend'], label='Trend', marker='o')\n",
    "plt.plot(df_reconstructed.index, df_reconstructed['resid'], label='Residual', marker='o')\n",
    "plt.plot(df_reconstructed.index, df_reconstructed['actual_values'], label='Actual Values', marker='o', linestyle='--')\n",
    "\n",
    "# customizing the plot\n",
    "plt.title('Time Series Decomposition')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Values')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "# showing the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ada876e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating an interactive multiline chart\n",
    "fig = px.line(df_reconstructed, labels={'value': 'Values'}, title='Time Series Decomposition')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c4d6be7",
   "metadata": {},
   "source": [
    "#### c) checking if data is stationary or not"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0e26a67",
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.tsa.stattools import adfuller\n",
    "\n",
    "# extractting the time series from DataFrame\n",
    "time_series = rows['AverageSentimentScore']\n",
    "\n",
    "# performing the ADF test\n",
    "result = adfuller(time_series)\n",
    "\n",
    "# extracting and print the results\n",
    "adf_statistic, p_value, used_lag, nobs, critical_values, icbest = result\n",
    "\n",
    "print(\"ADF Statistic:\", adf_statistic)\n",
    "print(\"p-value:\", p_value)\n",
    "print(\"Used Lag:\", used_lag)\n",
    "print(\"Number of Observations:\", nobs)\n",
    "print(\"Critical Values:\", critical_values)\n",
    "print(\"IC Best:\", icbest)\n",
    "\n",
    "# interpreting the results\n",
    "if p_value <= 0.05:\n",
    "    print(\"Reject the null hypothesis: The data is stationary.\")\n",
    "else:\n",
    "    print(\"Fail to reject the null hypothesis: The data is not stationary.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7907008",
   "metadata": {},
   "source": [
    "#### d) Reverse filling missing data & applying known informations to entire data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3767e5e6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "merged_data.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccd1143d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating a copy of the 'merged_data' DataFrame\n",
    "reversed_df = merged_data.copy()\n",
    "\n",
    "# reversing the order of the DataFrame\n",
    "reversed_df = reversed_df.iloc[::-1].reset_index(drop=True)\n",
    "reversed_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e755e83",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "reversed_df.tail(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8a507a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "end_date = pd.to_datetime(\"2009-06-08\")\n",
    "\n",
    "# slicing the df_reconstructed DataFrame to include only rows until the specified end_date\n",
    "composition = df_reconstructed.loc[:end_date].copy()\n",
    "\n",
    "\n",
    "# reversing the order of the DataFrame\n",
    "composition = composition.iloc[::-1].reset_index(drop=False)\n",
    "composition.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54d3850d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(composition)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "785aa3d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# finding the index where missing values start in reversed_df\n",
    "missing_start_idx = reversed_df[reversed_df['AverageSentimentScore'].isna()].index[0]\n",
    "\n",
    "# linear interpolation for available values in reversed_df\n",
    "reversed_df['AverageSentimentScore'] = reversed_df['AverageSentimentScore'].interpolate(method='linear')\n",
    "\n",
    "# iterating over the remaining missing values and backcast them based on composition\n",
    "for idx in range(missing_start_idx, len(reversed_df)):\n",
    "    \n",
    "    # calculating the corresponding date for the reversed_df index\n",
    "    date = reversed_df.index[idx]\n",
    "    \n",
    "    # finding the corresponding row in the composition DataFrame\n",
    "    composition_row = composition[composition['Date'] == date]\n",
    "    \n",
    "    # checking if a corresponding row was found\n",
    "    if not composition_row.empty:\n",
    "        \n",
    "        # using the trend and resid values to estimate the missing value\n",
    "        estimated_value = composition_row['trend'].values[0] + composition_row['resid'].values[0]\n",
    "        \n",
    "        # filling in the missing value in reversed_df\n",
    "        reversed_df.at[date, 'AverageSentimentScore'] = estimated_value\n",
    "\n",
    "# filling the remaining missing values with NaN (if any)\n",
    "reversed_df['AverageSentimentScore'].fillna(np.nan, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96efaee5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "reversed_df.tail(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb62c1b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_df = reversed_df.iloc[::-1]\n",
    "full_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a523b6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_df.reset_index(drop=True, inplace=True)\n",
    "full_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdccb170",
   "metadata": {},
   "outputs": [],
   "source": [
    "# converting the 'Date' column to a datetime data type\n",
    "full_df['Date'] = pd.to_datetime(full_df['Date'])\n",
    "\n",
    "# setting the 'Date' column as the index\n",
    "full_df.set_index('Date', inplace=True)\n",
    "\n",
    "# verifing the changes\n",
    "print(full_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "126b2dd0",
   "metadata": {},
   "source": [
    "### Stationarity check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5676726e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# linear interpolation for available values in reversed_df\n",
    "merged_data['AverageSentimentScore'] = merged_data['AverageSentimentScore'].interpolate(method='linear')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5677e77",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e10157e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.tsa.stattools import adfuller\n",
    "stationarity = adfuller(merged_data['AverageSentimentScore'])\n",
    "\n",
    "#stationarity\n",
    "print('Dickey Fuller p-value: %F' % stationarity[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ed862d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculating the second differenced series\n",
    "merged_data['Differenced_Sentiment'] = merged_data['AverageSentimentScore'].diff()\n",
    "\n",
    "# backfilling NaN values with zero\n",
    "merged_data['Differenced_Sentiment'].fillna(0, inplace=True)\n",
    "print(merged_data) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28bdca99",
   "metadata": {},
   "outputs": [],
   "source": [
    "stationarity = adfuller(merged_data['Differenced_Sentiment'])\n",
    "\n",
    "print('Dickey Fuller p-value: %F' % stationarity[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d85da0c",
   "metadata": {},
   "source": [
    "# 7 day forecast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2cb5481",
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.api as sm\n",
    "\n",
    "merged_data['Date'] = pd.to_datetime(merged_data['Date'])\n",
    "merged_data.set_index('Date', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e30072e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# splitting the data into a training and test set\n",
    "train_size = int(0.8 * len(merged_data))\n",
    "train_data = merged_data.iloc[:train_size]\n",
    "test_data = merged_data.iloc[train_size:]\n",
    "\n",
    "# fitting the ETS model to the training data\n",
    "ets_model = sm.tsa.ExponentialSmoothing(train_data['Differenced_Sentiment'], trend='add', seasonal='add', seasonal_periods=20)\n",
    "ets_result = ets_model.fit()\n",
    "\n",
    "# generating forecasts for the test set\n",
    "forecast_periods = len(test_data)\n",
    "forecast_values = ets_result.forecast(steps=forecast_periods)\n",
    "\n",
    "\n",
    "# calculating forecast errors\n",
    "forecast_errors = test_data['Differenced_Sentiment'] - forecast_values\n",
    "\n",
    "# calculate evaluation metrics\n",
    "mae_ets = forecast_errors.abs().mean()\n",
    "mse_ets = (forecast_errors ** 2).mean()\n",
    "rmse_ets = np.sqrt(mse_ets)\n",
    "mape_ets = (forecast_errors / test_data['Differenced_Sentiment']).abs().mean() * 100\n",
    "\n",
    "# displaing the results\n",
    "print(\"Forecast Evaluation Results:\")\n",
    "print(\"ETS_Mean Absolute Error (MAE):\", mae_ets)\n",
    "print(\"ETS_Mean Squared Error (MSE):\", mse_ets)\n",
    "print(\"ETS_Root Mean Squared Error (RMSE):\", rmse_ets)\n",
    "print(\"ETS_Mean Absolute Percentage Error (MAPE):\", mape_ets)\n",
    "\n",
    "\n",
    "# creating a DataFrame to store error results for ets model\n",
    "error_ets = pd.DataFrame({\n",
    "    'Model': ['ExponentialSmoothing'],\n",
    "    'MAE': [mae_ets],\n",
    "    'MSE': [mse_ets],\n",
    "    'RMSE': [rmse_ets],\n",
    "    'MAPE': [mape_ets]\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d56f072",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating a chart to visualize the results\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(train_data.index, train_data['Differenced_Sentiment'], label='Training Data', marker='o')\n",
    "plt.plot(test_data.index, test_data['Differenced_Sentiment'], label='Test Data', marker='o')\n",
    "plt.plot(test_data.index, forecast_values, label='Forecast', linestyle='--', marker='o')\n",
    "\n",
    "plt.legend()\n",
    "plt.title('Time Series Forecast with ETS')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Value')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a316e4fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fitting the ETS model to data\n",
    "ets_model = sm.tsa.ExponentialSmoothing(merged_data['Differenced_Sentiment'], trend='add', seasonal='add', seasonal_periods=7)\n",
    "ets_result = ets_model.fit()\n",
    "\n",
    "# generating forecasts for the next 7 days\n",
    "forecast_periods = 7\n",
    "forecast_values = ets_result.forecast(steps=forecast_periods)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a34ab3f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plotting the original data and the forecast\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(merged_data['Differenced_Sentiment'], label='Original Data', marker='o')\n",
    "plt.plot(forecast_values, label='Forecast', linestyle='--', marker='o')\n",
    "plt.legend()\n",
    "plt.title('7 Days Forecast')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Value')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7691752d",
   "metadata": {},
   "source": [
    "### 30 Days forecast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0234b32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fitting the ETS model to data\n",
    "ets_model = sm.tsa.ExponentialSmoothing(merged_data['Differenced_Sentiment'], trend='add', seasonal='add', seasonal_periods=10)\n",
    "ets_result = ets_model.fit()\n",
    "\n",
    "# generating forecasts for the next 30 days\n",
    "forecast_periods = 30\n",
    "forecast_values = ets_result.forecast(steps=forecast_periods)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56cddb1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plottinh the original data and the forecast\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(merged_data['Differenced_Sentiment'], label='Original Data', marker='o')\n",
    "plt.plot(forecast_values, label='Forecast', linestyle='--', marker='o')\n",
    "plt.legend()\n",
    "plt.title('30 Days Forecast')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Value')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf2e6e3a",
   "metadata": {},
   "source": [
    "## 90 days forecast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1ce1026",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fitting the ETS model to data\n",
    "ets_model = sm.tsa.ExponentialSmoothing(merged_data['Differenced_Sentiment'], trend='add', seasonal='add', seasonal_periods=30)\n",
    "ets_result = ets_model.fit()\n",
    "\n",
    "# generating forecasts for the next 90 days\n",
    "forecast_periods = 90\n",
    "forecast_values = ets_result.forecast(steps=forecast_periods)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4835caf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the original data and the forecast\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(merged_data['Differenced_Sentiment'], label='Original Data', marker='o')\n",
    "plt.plot(forecast_values, label='Forecast', linestyle='--', marker='o')\n",
    "plt.legend()\n",
    "plt.title('90 Days Forecast')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Value')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e1407ca",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "abd7a88e",
   "metadata": {},
   "source": [
    "# ARIMA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7548239",
   "metadata": {},
   "source": [
    "### Converting data into NonStationary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1b4f189",
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.tsa.stattools import adfuller\n",
    "stationarity = adfuller(full_df['AverageSentimentScore'])\n",
    "\n",
    "#stationarity\n",
    "print('Dickey Fuller p-value: %F' % stationarity[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4814f90c",
   "metadata": {},
   "outputs": [],
   "source": [
    "time_series = full_df['AverageSentimentScore']\n",
    "\n",
    "# performming the ADF test\n",
    "result = adfuller(time_series)\n",
    "\n",
    "# extractting and print the results\n",
    "adf_statistic, p_value, used_lag, nobs, critical_values, icbest = result\n",
    "\n",
    "print(\"ADF Statistic:\", adf_statistic)\n",
    "print(\"p-value:\", p_value)\n",
    "print(\"Used Lag:\", used_lag)\n",
    "print(\"Number of Observations:\", nobs)\n",
    "print(\"Critical Values:\", critical_values)\n",
    "print(\"IC Best:\", icbest)\n",
    "\n",
    "# interpreting the results\n",
    "if p_value <= 0.05:\n",
    "    print(\"Reject the null hypothesis: The data is stationary.\")\n",
    "else:\n",
    "    print(\"Fail to reject the null hypothesis: The data is not stationary.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8f99469",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# calculating differenced series\n",
    "full_df['Differenced_Sentiment'] = full_df['AverageSentimentScore'].diff()\n",
    "\n",
    "# backfill NaN values with zero or another suitable value\n",
    "full_df['Differenced_Sentiment'].fillna(0, inplace=True)\n",
    "print(full_df) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34b46364",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "stationarity = adfuller(full_df['Differenced_Sentiment'])\n",
    "\n",
    "#stationarity\n",
    "print('Dickey Fuller p-value: %F' % stationarity[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73431346",
   "metadata": {},
   "source": [
    "### Predicting next 7 days"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8fdd03e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# splitting the data\n",
    "train = full_df['AverageSentimentScore'].iloc[:-7] \n",
    "test = full_df['AverageSentimentScore'].iloc[-7:] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb7d6945",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from statsmodels.graphics.tsaplots import plot_acf, plot_pacf\n",
    "\n",
    "# calculating ACF and PACF\n",
    "acf_plot = plot_acf(full_df['Differenced_Sentiment'], lags=30)\n",
    "plt.title('Autocorrelation Function (ACF)')\n",
    "\n",
    "pacf_plot = plot_pacf(full_df['Differenced_Sentiment'], lags=30)\n",
    "plt.title('Partial Autocorrelation Function (PACF')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "345ea348",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import itertools\n",
    "import statsmodels.api as sm\n",
    "\n",
    "# defining the range of values for p, d, and q\n",
    "p_values = range(0, 3) \n",
    "d_values = range(0, 2)  \n",
    "q_values = range(0, 3)  \n",
    "\n",
    "best_aic = float(\"inf\")\n",
    "best_params = (0, 0, 0)\n",
    "\n",
    "# grid search to find the best parameters\n",
    "for p, d, q in itertools.product(p_values, d_values, q_values):\n",
    "    try:\n",
    "        model = sm.tsa.ARIMA(full_df['AverageSentimentScore'], order=(p, d, q))\n",
    "        results = model.fit()\n",
    "\n",
    "        aic = results.aic\n",
    "        if aic < best_aic:\n",
    "            best_aic = aic\n",
    "            best_params = (p, d, q)\n",
    "\n",
    "    except:\n",
    "        continue\n",
    "\n",
    "print(\"Best AIC:\", best_aic)\n",
    "print(\"Best Parameters (p, d, q):\", best_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92a75191",
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.api as sm\n",
    "\n",
    "# defining and fitting the ARIMA model\n",
    "model = sm.tsa.ARIMA(train, order=(0,1,1)) \n",
    "results = model.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07131350",
   "metadata": {},
   "outputs": [],
   "source": [
    "# generating forecasts for the 7 days\n",
    "forecast_values = results.forecast(steps=7)\n",
    "\n",
    "# printing forecasted values\n",
    "print(\"Forecasted Values:\", forecast_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d627ab9c",
   "metadata": {},
   "source": [
    "# Rolling method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63507d15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# converting the 'Date' column to datetime\n",
    "daily_average_sentiment['Date'] = pd.to_datetime(daily_average_sentiment['Date'])\n",
    "\n",
    "# finding the minimum and maximum dates in DataFrame\n",
    "min_date = daily_average_sentiment['Date'].min()\n",
    "max_date = daily_average_sentiment['Date'].max()\n",
    "\n",
    "# creating a date range covering the entire range\n",
    "date_range = pd.date_range(start=min_date, end=max_date)\n",
    "\n",
    "# creating a new DataFrame with the date range\n",
    "date_range_df = pd.DataFrame({'Date': date_range})\n",
    "\n",
    "# merging the date range DataFrame with'daily_average_sentiment' DataFrame, filling missing dates\n",
    "rolling = date_range_df.merge(daily_average_sentiment, on='Date', how='left')\n",
    "\n",
    "# setting 'AverageSentimentScore' to NaN for missing dates\n",
    "rolling['AverageSentimentScore'].fillna(float('nan'), inplace=True)\n",
    "\n",
    "# sortting the merged DataFrame by date\n",
    "rolling.sort_values('Date', inplace=True)\n",
    "\n",
    "# resetting the index\n",
    "rolling.reset_index(drop=True, inplace=True)\n",
    "rolling.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d966b90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# linear interpolation for available values in reversed_df\n",
    "rolling['AverageSentimentScore'] = rolling['AverageSentimentScore'].interpolate(method='linear')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db70bea5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculating the differenced series\n",
    "rolling['Differenced_Sentiment'] = rolling['AverageSentimentScore'].diff()\n",
    "\n",
    "# backfilling NaN values with zero or another suitable value\n",
    "rolling['Differenced_Sentiment'].fillna(0, inplace=True)\n",
    "print(rolling) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc690e95",
   "metadata": {},
   "outputs": [],
   "source": [
    "rolling['Date'] = pd.to_datetime(rolling['Date'], format='%Y/%m/%d')\n",
    "rolling = rolling.set_index('Date')\n",
    "rolling = rolling.rename(columns={'Differenced_Sentiment': 'y'})\n",
    "\n",
    "rolling = rolling.sort_index()\n",
    "rolling.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbf3ee81",
   "metadata": {},
   "outputs": [],
   "source": [
    "rolling.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c85515c",
   "metadata": {},
   "outputs": [],
   "source": [
    "(rolling.index == pd.date_range(start=rolling.index.min(),\n",
    "                             end=rolling.index.max(),\n",
    "                             freq=rolling.index.freq)).all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1596e9d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# splitting data into train-test\n",
    "steps = 7\n",
    "rolling_train = rolling[:-steps]\n",
    "rolling_test  = rolling[-steps:]\n",
    "\n",
    "print(f\"Train dates : {rolling_train.index.min()} --- {rolling_train.index.max()}  (n={len(rolling_train)})\")\n",
    "print(f\"Test dates  : {rolling_test.index.min()} --- {rolling_test.index.max()}  (n={len(rolling_test)})\")\n",
    "\n",
    "fig, ax=plt.subplots(figsize=(9, 4))\n",
    "rolling_train['y'].plot(ax=ax, label='train')\n",
    "rolling_test['y'].plot(ax=ax, label='test')\n",
    "ax.legend();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70bf377b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating and train forecaster\n",
    "from skforecast.ForecasterAutoreg import ForecasterAutoreg\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "forecaster = ForecasterAutoreg(\n",
    "                regressor = RandomForestRegressor(random_state=123),\n",
    "                lags = 7\n",
    "                )\n",
    "\n",
    "forecaster.fit(y=rolling_train['y'])\n",
    "forecaster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ae48550",
   "metadata": {},
   "outputs": [],
   "source": [
    "# predictions\n",
    "steps = 7\n",
    "predictions = forecaster.predict(steps=steps)\n",
    "predictions.index = rolling_test.index\n",
    "predictions.head(7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f82ad062",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plotting\n",
    "fig, ax = plt.subplots(figsize=(9, 4))\n",
    "rolling_train['y'].plot(ax=ax, label='train')\n",
    "rolling_test['y'].plot(ax=ax, label='test')\n",
    "predictions.plot(ax=ax, label='predictions')\n",
    "ax.legend();\n",
    "\n",
    "\n",
    "# calculating forecast errors\n",
    "errors = rolling_test['y'] - predictions\n",
    "\n",
    "# calculating evaluation metrics\n",
    "mae_R = mean_absolute_error(rolling_test['y'], predictions)\n",
    "mse_R = mean_squared_error(rolling_test['y'], predictions)\n",
    "rmse_R = np.sqrt(mse)\n",
    "mape_R = (np.abs(errors) / rolling_test['y']).mean() * 100\n",
    "\n",
    "# printing the error metrics\n",
    "print(\"Roll_Mean Absolute Error (MAE):\", mae_R)\n",
    "print(\"Roll_Mean Squared Error (MSE):\", mse_R)\n",
    "print(\"Roll_Root Mean Squared Error (RMSE):\", rmse_R)\n",
    "print(\"Roll_Mean Absolute Percentage Error (MAPE):\", mape_R)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eef2cbf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating a DataFrame to store error results for Random forest model\n",
    "error_R = pd.DataFrame({\n",
    "    'Model': ['RandomForestRegressor'],\n",
    "    'MAE': [mae_R],\n",
    "    'MSE': [mse_R],\n",
    "    'RMSE': [rmse_R],\n",
    "    'MAPE': [mape_R]\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cc3c375",
   "metadata": {},
   "outputs": [],
   "source": [
    "error_df = pd.concat([error_ets, error_R], ignore_index=True)\n",
    "error_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d84585cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "forecast_data = rolling.copy()\n",
    "\n",
    "# setting the frequency of the DatetimeIndex\n",
    "forecast_data.index.freq = 'D'\n",
    "\n",
    "# setting the number of periods to forecast\n",
    "forecast_periods = 7\n",
    "\n",
    "# creating a RandomForestRegressor-based forecaster\n",
    "forecaster = ForecasterAutoreg(\n",
    "    regressor=RandomForestRegressor(random_state=123),\n",
    "    lags=7\n",
    ")\n",
    "\n",
    "# fitting the forecaster to data\n",
    "forecaster.fit(y=forecast_data['y'])\n",
    "\n",
    "# generating forecasts for the next 7 days\n",
    "forecasts = forecaster.predict(steps=forecast_periods)\n",
    "\n",
    "# getting the last date in the existing data\n",
    "last_date = forecast_data.index[-1]\n",
    "\n",
    "# creating a date range for the forecasted dates\n",
    "forecast_dates = pd.date_range(start=last_date, periods=forecast_periods + 1, closed='right')\n",
    "\n",
    "# creating a DataFrame for the forecasts\n",
    "forecast_df = pd.DataFrame({'Forecast': forecasts}, index=forecast_dates[1:])\n",
    "\n",
    "# printing the forecast\n",
    "print(forecast_df)\n",
    "\n",
    "\n",
    "# plotting the forecast\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(forecast_data.index, forecast_data['y'], label='Actual Data', marker='o')\n",
    "plt.plot(forecast_df.index, forecast_df['Forecast'], label='Forecast', linestyle='--', marker='o')\n",
    "plt.title('Forecast for the Next 7 Days')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Value')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91189b92",
   "metadata": {},
   "source": [
    "## Dashboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "effd7d77",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1902f11",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ipywidgets as widgets\n",
    "from IPython.display import display\n",
    "import ipywidgets as widgets\n",
    "import plotly.express as px\n",
    "import statsmodels.api as sm\n",
    "import plotly.graph_objects as go\n",
    "from jupyter_dash import JupyterDash\n",
    "import dash_core_components as dcc\n",
    "import dash_html_components as html\n",
    "from dash.dependencies import Input, Output\n",
    "import dash_table\n",
    "import dash_bootstrap_components as dbc\n",
    "import plotly.offline as pyo\n",
    "from dash.dependencies import Output, Input\n",
    "import dash_core_components as dcc\n",
    "import dash_html_components as html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24db3eea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the horizontal bar chart\n",
    "fig_bar = px.bar(df4, x='SentimentScore', y='DayOfWeek', orientation='h', title='Average Sentiment Score per Day of the Week')\n",
    "fig_bar.update_layout(xaxis_title='Average Sentiment Score', yaxis_title='Day of the Week', showlegend=False)\n",
    "\n",
    "\n",
    "\n",
    "# Creating an interactive box plot\n",
    "fig_box = px.box(df, x=df['Date'].dt.date, y='SentimentScore', labels={'x': 'Date', 'y': 'Sentiment Score'},\n",
    "                 title='Distribution of Sentiment Scores for Each Day')\n",
    "fig_box.update_xaxes(tickangle=90)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Existing code for the first ETS forecasting chart\n",
    "ets_model1 = sm.tsa.ExponentialSmoothing(merged_data['Differenced_Sentiment'], trend='add', seasonal='add', seasonal_periods=7)\n",
    "ets_result1 = ets_model1.fit()\n",
    "forecast_periods1 = 7\n",
    "forecast_values1 = ets_result1.forecast(steps=forecast_periods1)\n",
    "fig_ets1 = go.Figure()\n",
    "fig_ets1.add_trace(go.Scatter(x=merged_data['Date'], y=merged_data['Differenced_Sentiment'], mode='lines+markers', name='Original Data'))\n",
    "forecast_dates1 = [merged_data['Date'].iloc[-1]] + [(pd.to_datetime(merged_data['Date'].iloc[-1]) + pd.DateOffset(days=i)).date() for i in range(1, forecast_periods1 + 1)]\n",
    "fig_ets1.add_trace(go.Scatter(x=forecast_dates1,\n",
    "                            y=[merged_data['Differenced_Sentiment'].iloc[-1]] + forecast_values1.tolist(),\n",
    "                            mode='lines+markers', name='7 Days Forecast', line=dict(dash='dash')))\n",
    "fig_ets1.update_layout(title='7 Days Forecast', xaxis_title='Date', yaxis_title='Value')\n",
    "\n",
    "\n",
    "\n",
    "# New code for the second ETS forecasting chart\n",
    "ets_model2 = sm.tsa.ExponentialSmoothing(merged_data['Differenced_Sentiment'], trend='add', seasonal='add', seasonal_periods=10)\n",
    "ets_result2 = ets_model2.fit()\n",
    "forecast_periods2 = 30\n",
    "forecast_values2 = ets_result2.forecast(steps=forecast_periods2)\n",
    "fig_ets2 = go.Figure()\n",
    "fig_ets2.add_trace(go.Scatter(x=merged_data['Date'], y=merged_data['Differenced_Sentiment'], mode='lines+markers', name='Original Data'))\n",
    "forecast_dates2 = [merged_data['Date'].iloc[-1]] + [(pd.to_datetime(merged_data['Date'].iloc[-1]) + pd.DateOffset(days=i)).date() for i in range(1, forecast_periods2 + 1)]\n",
    "fig_ets2.add_trace(go.Scatter(x=forecast_dates2,\n",
    "                            y=[merged_data['Differenced_Sentiment'].iloc[-1]] + forecast_values2.tolist(),\n",
    "                            mode='lines+markers', name='30 Days Forecast', line=dict(dash='dash')))\n",
    "fig_ets2.update_layout(title='30 Days Forecast', xaxis_title='Date', yaxis_title='Value')\n",
    "\n",
    "\n",
    "\n",
    "# New code for the third ETS forecasting chart\n",
    "ets_model3 = sm.tsa.ExponentialSmoothing(merged_data['Differenced_Sentiment'], trend='add', seasonal='add', seasonal_periods=30)\n",
    "ets_result3 = ets_model3.fit()\n",
    "forecast_periods3 = 90\n",
    "forecast_values3 = ets_result3.forecast(steps=forecast_periods3)\n",
    "fig_ets3 = go.Figure()\n",
    "fig_ets3.add_trace(go.Scatter(x=merged_data['Date'], y=merged_data['Differenced_Sentiment'], mode='lines+markers', name='Original Data'))\n",
    "forecast_dates3 = [merged_data['Date'].iloc[-1]] + [(pd.to_datetime(merged_data['Date'].iloc[-1]) + pd.DateOffset(days=i)).date() for i in range(1, forecast_periods3 + 1)]\n",
    "fig_ets3.add_trace(go.Scatter(x=forecast_dates3,\n",
    "                            y=[merged_data['Differenced_Sentiment'].iloc[-1]] + forecast_values3.tolist(),\n",
    "                            mode='lines+markers', name='90 Days Forecast', line=dict(dash='dash')))\n",
    "fig_ets3.update_layout(title='90 Days Forecast', xaxis_title='Date', yaxis_title='Value')\n",
    "\n",
    "\n",
    "data = {\n",
    "    'DayOfWeek': ['Friday', 'Monday', 'Saturday', 'Sunday', 'Thursday', 'Tuesday', 'Wednesday'],\n",
    "    'SentimentScore': [0.140448, 0.160366, 0.152299, 0.177131, 0.039275, 0.115775, 0.055170]\n",
    "}\n",
    "df_table = pd.DataFrame(data)\n",
    "\n",
    "\n",
    "# Create the table\n",
    "table = dash_table.DataTable(\n",
    "    id='sentiment-table',\n",
    "    columns=[{'name': col, 'id': col} for col in df_table.columns],\n",
    "    data=df_table.to_dict('records'),\n",
    "    style_table={\n",
    "        'width': '50%',\n",
    "        'margin': 'auto'\n",
    "    }\n",
    ")\n",
    "\n",
    "# Create the horizontal bar chart\n",
    "fig_bar = px.bar(df_table, x='SentimentScore', y='DayOfWeek', orientation='h',\n",
    "                 title='Average Sentiment Score per Day of the Week')\n",
    "fig_bar.update_layout(\n",
    "    xaxis_title='Average Sentiment Score',\n",
    "    yaxis_title='Day of the Week',\n",
    "    showlegend=False,\n",
    "    plot_bgcolor='lightgray',  # Background color\n",
    "    paper_bgcolor='black'  # Plot area background color\n",
    ")\n",
    "\n",
    "# Creating the dashboard app\n",
    "app = dash.Dash(__name__)\n",
    "\n",
    "# Define a consistent style for graphs\n",
    "graph_style = {\n",
    "    'border': '1px solid lightgray',\n",
    "    'margin': '10px',\n",
    "    'box-shadow': '2px 2px 5px #888888'\n",
    "}\n",
    "\n",
    "# Modify the layout to include the customized charts\n",
    "app.layout = html.Div([\n",
    "    html.H1('Twitter Sentiment Analysis and Forecast', style={'textAlign': 'center'}),\n",
    "    dcc.Graph(figure=fig_box, style=graph_style),\n",
    "    dcc.Graph(figure=fig_bar, style=graph_style),\n",
    "    dcc.Graph(figure=fig_ets1, style=graph_style),\n",
    "    dcc.Graph(figure=fig_ets2, style=graph_style),\n",
    "    dcc.Graph(figure=fig_ets3, style=graph_style),\n",
    "    table,\n",
    "], style={'padding': '20px'})\n",
    "\n",
    "\n",
    "@app.callback(Output('dummy-div', 'children'), Input('save-button', 'n_clicks'))\n",
    "def save_dashboard(n_clicks):\n",
    "    if n_clicks:\n",
    "        with open('C:\\\\ESTERA\\\\CCT\\\\TWEETS\\\\dash_app.html', 'w') as f:\n",
    "            f.write(app.index())\n",
    "    return None\n",
    "\n",
    "# Create a hidden button and a dummy div\n",
    "html.Button(id='save-button', n_clicks=0, style={'display': 'none'}),\n",
    "html.Div(id='dummy-div'),\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    app.run_server(debug=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08f52950",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a7c306b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
